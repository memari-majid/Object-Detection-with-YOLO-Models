"""
Dataset Cleaning Tool

This module provides functionality to create a clean dataset by removing difficult or problematic
images identified through previous analysis. It processes the original dataset and creates a new
clean version with corresponding configuration files.

Key Features:
- Removes difficult images identified by analysis
- Creates new dataset files for train/val/test splits
- Generates updated YAML configuration for the clean dataset
- Provides statistical comparison between original and clean datasets

Dependencies:
    - pandas
    - rich
    - pyyaml
    - pathlib
"""

import os
from pathlib import Path
import pandas as pd
from rich.console import Console
import shutil

console = Console()

def load_difficult_images():
    """
    Load the list of difficult images from previous analysis results.
    
    Returns:
        set: A set of image paths identified as difficult, or None if the analysis file is not found.
        
    Note:
        Requires 'difficult_images_analysis.csv' to be present in the difficult_images_analysis directory.
        This file should be generated by running find_difficult_images.py first.
    """
    analysis_file = Path('difficult_images_analysis/difficult_images_analysis.csv')
    if not analysis_file.exists():
        console.print("[red]Error: Difficult images analysis file not found. Run find_difficult_images.py first.")
        return None
    
    df = pd.read_csv(analysis_file)
    return set(df['image_path'].tolist())

def create_clean_dataset():
    """
    Create a new dataset excluding difficult images.
    
    This function:
    1. Loads the list of difficult images
    2. Creates a new directory for clean dataset
    3. Processes train/val/test splits separately
    4. Generates new data files excluding difficult images
    5. Creates an updated YAML configuration file
    
    Returns:
        bool: True if clean dataset was created successfully, False otherwise.
    
    Directory Structure Created:
    data_clean/
    ├── train_clean.txt
    ├── val_clean.txt
    ├── test_clean.txt
    └── data_clean.yml
    """
    try:
        # Load difficult images
        difficult_images = load_difficult_images()
        if difficult_images is None:
            return False
            
        # Create directories for clean dataset
        clean_data_dir = Path('data_clean')
        clean_data_dir.mkdir(exist_ok=True)
        
        # Process each split (train, val, test)
        for split in ['train', 'val', 'test']:
            grass_file = f'data/{split}_grass.txt'
            if not os.path.exists(grass_file):
                console.print(f"[yellow]Warning: {grass_file} not found, skipping...")
                continue
            
            # Read original grass dataset
            with open(grass_file, 'r') as f:
                image_paths = [line.strip() for line in f.readlines()]
            
            # Filter out difficult images
            clean_images = [path for path in image_paths if path not in difficult_images]
            
            # Create new file for clean dataset
            clean_file = clean_data_dir / f'{split}_clean.txt'
            with open(clean_file, 'w') as f:
                f.write('\n'.join(clean_images))
            
            console.print(f"[green]Created clean {split} file: {len(clean_images)} images " +
                        f"(removed {len(image_paths) - len(clean_images)} difficult images)")
        
        # Create new data.yml for clean dataset
        create_clean_data_yaml(clean_data_dir)
        
        return True
        
    except Exception as e:
        console.print(f"[red]Error creating clean dataset: {str(e)}")
        return False

def create_clean_data_yaml(clean_data_dir):
    """
    Create a new data.yml file for the clean dataset.
    
    Args:
        clean_data_dir (Path): Directory where clean dataset files are stored.
    
    The function copies the configuration from the original data_grass.yml
    and updates the paths to point to the new clean dataset files.
    
    Note:
        Requires original data_grass.yml to be present in the data directory.
    """
    try:
        # Read original grass data.yml
        with open('data/data_grass.yml', 'r') as f:
            import yaml
            grass_config = yaml.safe_load(f)
        
        # Update paths for clean dataset
        clean_config = grass_config.copy()
        clean_config['train'] = str(Path.cwd() / clean_data_dir / 'train_clean.txt')
        clean_config['val'] = str(Path.cwd() / clean_data_dir / 'val_clean.txt')
        clean_config['test'] = str(Path.cwd() / clean_data_dir / 'test_clean.txt')
        
        # Save new clean data.yml
        clean_yaml_path = clean_data_dir / 'data_clean.yml'
        with open(clean_yaml_path, 'w') as f:
            yaml.dump(clean_config, f, default_flow_style=False)
        
        console.print(f"[green]Created clean data.yml at {clean_yaml_path}")
        
    except Exception as e:
        console.print(f"[red]Error creating clean data.yml: {str(e)}")

def print_dataset_statistics():
    """
    Print comparative statistics about the original and clean datasets.
    
    Displays:
    - Number of images in each split (train/val/test) for both datasets
    - Total number of images in each dataset
    - Percentage and absolute reduction in dataset size
    
    Note:
        This function assumes both original and clean dataset files exist
        in their respective directories.
    """
    try:
        stats = {
            'original': {},
            'clean': {}
        }
        
        # Collect statistics for both datasets
        for dataset_type in ['original', 'clean']:
            prefix = 'data_clean' if dataset_type == 'clean' else 'data'
            suffix = '_clean.txt' if dataset_type == 'clean' else '_grass.txt'
            
            for split in ['train', 'val', 'test']:
                file_path = Path(prefix) / f'{split}{suffix}'
                if file_path.exists():
                    with open(file_path, 'r') as f:
                        stats[dataset_type][split] = len(f.readlines())
        
        # Print statistics
        console.print("\n[yellow]Dataset Statistics:")
        console.print("\nOriginal Grass Dataset:")
        for split, count in stats['original'].items():
            console.print(f"{split}: {count} images")
        
        console.print("\nClean Dataset:")
        for split, count in stats['clean'].items():
            console.print(f"{split}: {count} images")
            
        # Calculate total reduction
        orig_total = sum(stats['original'].values())
        clean_total = sum(stats['clean'].values())
        reduction = ((orig_total - clean_total) / orig_total) * 100
        
        console.print(f"\nTotal reduction: {reduction:.1f}% " +
                     f"({orig_total - clean_total} images removed)")
        
    except Exception as e:
        console.print(f"[red]Error calculating statistics: {str(e)}")

if __name__ == "__main__":
    console.print("[bold blue]Creating clean dataset...")
    
    if create_clean_dataset():
        print_dataset_statistics()
        console.print("\n[bold green]Clean dataset created successfully!")
        console.print("You can now use data_clean/data_clean.yml for training with the cleaned dataset.")
    else:
        console.print("[bold red]Failed to create clean dataset.")